{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>This Machine Learning Model will Attempt to Predict the Type of Bicycle Used from Trip Data</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data path\n",
    "data_path = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base data to dataframe\n",
    "all_data_file = 'all_data.csv'\n",
    "all_df = pd.read_csv(os.path.join(data_path, all_data_file))\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change NaN values to 0 - only occurs on hours column so this is accurate\n",
    "all_df['Hours'] = all_df['Hours'].fillna(0)\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning Index into Orig_Index Column to trace back to original records\n",
    "all_df.reset_index(inplace=True)\n",
    "all_df = all_df.rename(columns = {'index': 'Orig_Index'})\n",
    "all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining fields to use\n",
    "columns = ['Miles', 'Duration', 'Speed','Type']\n",
    "# target = ['Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataframe with desired columns\n",
    "bike_df = all_df.loc[:,columns].copy()\n",
    "bike_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting out Features and Target\n",
    "y = bike_df[\"Type\"]\n",
    "X = bike_df.drop(columns=\"Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting out Training and Testing data\n",
    "# from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking training data shape\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking testing data shape\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Logistic Regression Model\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(solver='lbfgs', max_iter=200, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Predictions\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy score\n",
    "# from sklearn.metrics import accuracy_score\n",
    "accu_score = accuracy_score(y_test, y_pred)\n",
    "print(accu_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Predictions to DataFrame\n",
    "pred_df = pd.DataFrame(y_pred)\n",
    "pred_df.reset_index()\n",
    "pred_df = pred_df.rename(columns = {0:'Prediction'})\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tests to DataFrame\n",
    "test_df = pd.DataFrame(y_test)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making index into a column\n",
    "test_df.reset_index(inplace=True)\n",
    "test_df = test_df.rename(columns = {'index': 'Orig_Index'})\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the new test and predition dataframes horizontally for comparison\n",
    "test_pred_df = pd.concat([test_df, pred_df], axis = 1)\n",
    "test_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify records where prediction is wrong\n",
    "pred_errs_df = test_pred_df.loc[test_pred_df['Type'] != test_pred_df['Prediction']]\n",
    "pred_errs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging pred_errs_df with all_df to see details of errors\n",
    "err_details_df = pd.merge(pred_errs_df, all_df, on=[\"Orig_Index\", \"Orig_Index\"])\n",
    "err_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up err_details_df to eliminate redundancy and improve readability\n",
    "err_details_df.drop(columns='Type_y', inplace=True)\n",
    "err_details_df.rename(columns = {'Type_x' : 'Type'}, inplace=True)\n",
    "err_details_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print formatted Accuracy Score\n",
    "print('Accruacy is: {:.2f}'.format(accu_score * 100) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
